CSDN：[反向传播算法的直观理解](https://blog.csdn.net/mao_xiao_feng/article/details/53048213)

知乎：[AI从入门到放弃：BP神经网络算法推导及代码实现笔记](https://zhuanlan.zhihu.com/p/38006693)

看完两篇文章之后，对常见的神经网络代码运行过程的理解，以 MNIST 为例子：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190302204703.png)

上面这个是没有使用 CNN 网络模型实现的。

**其中可以看到框出来的代码，表示每次读取 batch_size=100 张图片作为输入，即100x28x28 作为输入 --> 进行 n_batch =55000//100 个次 --> 结束之后就把所有数据训练了一遍 --> 再把如上过程进行了22 个epoch（即22个轮回）。**

如果该过程拿到常见的**神经网络模型代码来讲**，大概是这样的：

1. 代码开始运行，整个神经网络的权值参数（即 filter 参数）会进行一次初始化；

2. **【进行第一个epoch】**进行第一个 batch_size 的数据作为输入，**进行一次正向传播（使用第一步初始化的参数值）**，得到的结果再与真实数据（与label）比较"差距"，理解为损失函数 loss，接下来就要最小化 loss 的值，怎么最小化呢？

3. 进行反向传播，反向传播的细节参考最前面两篇文章看看。【我记录下重点】以开头第一篇文章为例，反向传播之后，最后更新参数，如下：

   ![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190302205000.png)

   这样误差反向传播法就完成了，这样我们得到了一**组新的参数值**（而不是之前最开始初始化的参数值）。

4. 再进行第二个 batch_size 的数据作为输入，**进行一次正向传播（注意：这里使用的是上一步得到的新的参数值）**，得到的结果再与真实数据（与label）进行比较"差距"，最小化 loss，同上过程一样，再然后进行反向传播，**得到新的的参数值；**

5. 进行第三个、第四个、第五个 batch_size...... 直到所有数据都有作为输入数据；

6. **【再进行第二个epoch】**在这个过程的的第一个 batch_size 过程中使用的参数为：**第一个 epoch 中的最后一个 batch_size 后得到的参数进行正向传播**。

7. **【再进行第三、第四、第五个epoch。。。。。】**直到结束！

8. 注意：其实这里也有可能在第一个 epoch 完毕，得到的参数就是最优的。

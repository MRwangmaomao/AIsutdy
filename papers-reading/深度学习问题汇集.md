#### (1) 为什么全连接层的输入需要固定维度？

解答一：

我们知道SPP-Net的提出一方面解决了固定的输入图像大小问题，即输入图像大小不受限制。因为SPP-Net解决了全连接层的输入需要固定维度的问题。

那么为什么全连接层的输入需要固定维度呢？

答：全连接层的计算其实相当于输入的特征图数据矩阵和全连接层权值矩阵进行内积。在配置一个网络时，全连接层的参数维度是固定的，所以两个矩阵要能够进行内积，则输入的特征图的数据矩阵维数也需要固定。

以下图为例说明：

![](https://img-blog.csdn.net/20170617090438471?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdjFfdml2aWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

作为全连接层，如果输入的x维数不等，那么参数w肯定也会不同，因此，全连接层是必须确定输入，输出个数的。

解答二：

我们来看看。假设输入图片的大小是 100x100，经过 5 个卷积核 3x3 之后会差生 5x98x98 的 feature maps，就算你的输入图片的大小变成 102x102，那我的 feature maps 就是 5x100x100。这里的 feature maps 经过 2x2 的 polling 之后得到的是 25x25 和 26x26。没什么影响，这里的卷积核的大小是固定的，可以去卷积任何大小的图片。但是全连接层就不同了。假设最后一个卷积层有 50 个输出，下一层的全链接有 1000 个输入，那么这个链接矩阵就是 50x1000，哈哈，你想想，如果这里每次的输入图片大小都不一样，到这里如何进行链接呢？因为不同的图片大小经过最后一个卷积层的输入到输出之后压根就不可能都是 50 啊。这就是我们为什么要在全连接层这人进行操作的原因。



参考：

- [1]  https://blog.csdn.net/liyaohhh/article/details/50614380
- [2]  https://blog.csdn.net/v1_vivian/article/details/73275259



#### (2) 什么是空间金字塔池化（SPP）？

解答一：

![](https://img-blog.csdn.net/20160131154728544)

如上图所示，当我们输入一张图片的时候，我们利用不同大小的刻度，对一张图片进行了划分。上面示意图中，利用了三种不同大小的刻度（4x4,2x2,1x1），对一张输入的图片进行了划分，最后总共可以得到16+4+1=21个块，我们即将从这21个块中，每个块提取出一个特征，这样刚好就是我们要提取的21维特征向量。

第一张图片,我们把一张完整的图片，分成了 16 个块，也就是每个块的大小就是(w/4,h/4);

第二张图片，划分了4个块，每个块的大小就是(w/2,h/2);

第三张图片，把一整张图片作为了一个块，也就是块的大小为(w,h)。

空间金字塔最大池化的过程，其实就是从这 21 个图片块(bin)中，分别计算每个块的最大值，从而得到一个输出神经元。最后把一张任意大小的图片转换成了一个固定大小的21维特征（当然你可以设计其它维数的输出，增加金字塔的层数，或者改变划分网格的大小）。上面的三种不同刻度的划分，每一种刻度我们称之为：金字塔的一层，每一个图片块大小我们称之为：windows size 了。如果你希望，金字塔的某一层输出nxn个特征，那么你就要用windows size大小为：(w/n,h/n) 进行池化了。[3]

解答二：[4]

不管最后一个卷积层得到的特征图（feature maps）的大小，都可将其转化为了（4x4+2x2+1x1）x256的全连接层，也就是这些特征图的大小不同，但通道是相同的，那么如何将不同大小的特征图进行spp 呢？

假设输入的大小为axaxc，然后呢，这些特征图分别被分成了[1x1,2x2,4x4]大小的块，期望的输出为1x1xc,2x2xc,4x4xc,变形为（1x1+2x2+4x4）xc的二维数组，这全部是通过池化操作实现的，不过池化层的size和stride是不同的，具体有如下：

输入为[a,a]，输出为[n,n]，那么pool_size=⌈ n/a ⌉ ，stride=⌊ n/a⌋ ，这样我们就将其转化为了nxnxc的矩阵，例如13x13、10x10要转化为4x4的大小，那么采用[p_s=4,,s=3],[p_s=3,s=2]的池化操作后便可以得到。

如果原图输入是227x227，对于conv5出来后的输出，是13x13x256的，可以理解成有256个这样的filter，每个filter对应一张13x13的激活图。

如果像上图那样将激活图池化成4x4 2x2 1x1三张子图，做max pooling后，出来的特征就是固定长度的(16+4+1)x256那么多的维度了.如果原图的输入不是227x227，出来的特征依然是(16+4+1)x256；直觉地说，可以理解成将原来固定大小为(3x3)窗口的pool5改成了自适应窗口大小，窗口的大小和激活成比例，保证了经过pooling后出来的feature的长度是一致的。

如果要金字塔的某一层输出n x n个特征，只需要用窗口大小为：(w/n,h/n)进行池化即可。

当我们有很多层网络的时候，网络输入的是一张任意大小的图片，这个时候我们可以一直进行卷积、池化，直到即将与全连接层连接的时候，就要使用金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的奥妙之处！

参考：

- \[1] [空间金字塔池化SPP（Spatial Pyramid Pooling）](https://blog.csdn.net/g11d111/article/details/80789538)
- \[2] [深度学习笔记（一）空间金字塔池化阅读笔记Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://blog.csdn.net/liyaohhh/article/details/50614380)
- \[3] [空间金字塔池化(Spatial Pyramid Pooling, SPP)原理和代码实现(Pytorch)](http://www.cnblogs.com/marsggbo/p/8572846.html) （这篇看看~）
- [4] 微信文章：[空间金字塔池化SPP改进RCNN的重要思想](https://mp.weixin.qq.com/s?src=11&timestamp=1545365021&ver=1283&signature=rWFsCziQP5PCBy-HunyClPhvluLRCRasTkjeAcpOON2881JBWpv6DrBfIgPsSwHk*sseui9M4SwKZ135gGwjRi8Ek3mwpJMQntb5wuea5KK7hCACbIn7jYbMVFlxnMAl&new=1) （关于 SPP 的具体实现，这篇一定认真看下~）



#### (3) 空间金字塔池化具体实现更详细的描述？



![](https://img-blog.csdn.net/20180624105237393?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2cxMWQxMTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上图是原文中给出的示意图，需要从下往上看：

- 首先是输入层(input image)，其大小可以是任意的
- 进行卷积运算，到最后一个卷积层（图中是 conv5）输出得到该层的特征映射(feature maps)，其大小也是任意的
- 下面进入 SPP 层：
  - 我们先看最左边有 16 个蓝色小格子的图，它的意思是将从 conv5 得到的特征映射分成 16 份，另外 16X256 中的 256 表示的是 channel，即 SPP 对每一层都分成 16 份(不一定是等比分，原因看后面的内容就能理解了)。
  - 中间的 4 个绿色小格子和右边 1 个紫色大格子也同理，即将特征映射分别分成 4X256 和 1X256 份

到底怎么该理解呢？先看网上的资料：

- 蓝色的图1——我们把一张完整的图片，分成了 16 个块，也就是每个块的大小就是(w/4, h/4);
- 绿色的图2，划分了 4 个块，每个块的大小就是(w/2, h/2);
- 黑色的图3，把整张图片作为了 1 个块，也就是块的大小为(w, h)

空间金字塔最大池化的过程，其实就是从这 21 个图片块中，分别计算每个块的**最大值**（局部 max-pooling）。通过 SPP，我们就把一张任意大小的图片转换成了一个**固定大小的21维特征**（当然你可以设计其它维数的输出，增加金字塔的层数，或者改变划分网格的大小）。

上面的三种不同刻度的划分，每一种刻度我们称之为：金字塔的一层，每一个图片**块大小**我们称之为： windows size。如果你希望，金字塔的某一层输出 nxn 个特征，那么你就要用 windows size 大小为：(w/n, h/n)进行池化了。

![](https://img-blog.csdn.net/20170617091103341?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdjFfdml2aWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

参考：(2) 中的 [1]、[3]

SPP 的具体实现到底是怎样的？

假设输入的大小为`axaxc`，然后呢，这些特征图分别被分成了(1x1=)1、(2x2=)4、(4x4=)16 数量的块，期望的输出为 1x1xc、2x2xc、4x4xc、变形为（1x1+2x2+4x4）xc 的二维数组，这全部是通过池化操作实现的，不过池化层的 size 和 stride 是不同的，具体有如下：

> 输入为[a,a]，输出为[n,n]，那么pool_size=⌈ n/a ⌉ ，stride=⌊ n/a⌋ ，这样我们就将其转化为了 nxnxc 的矩阵，例如 13x13、10x10 要转化为 4x4 的大小，那么采用 [p_s=4, s=3]、[p_s=3, s=2] 的池化操作后便可以得到。

更多参考该篇微信文章：[空间金字塔池化SPP改进RCNN的重要思想](https://mp.weixin.qq.com/s?src=11&timestamp=1545365021&ver=1283&signature=rWFsCziQP5PCBy-HunyClPhvluLRCRasTkjeAcpOON2881JBWpv6DrBfIgPsSwHk*sseui9M4SwKZ135gGwjRi8Ek3mwpJMQntb5wuea5KK7hCACbIn7jYbMVFlxnMAl&new=1) 





